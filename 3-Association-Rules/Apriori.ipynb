{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Association Rule Mining\n",
    "\n",
    "\n",
    "by [__Michael Granitzer__ (michael.granitzer@uni-passau.de)]( http://www.mendeley.com/profiles/michael-granitzer/) and [Konstantin Ziegler (konstantin.ziegler@uni-passau.de)](http://zieglerk.net) based on examples from the [scikit-learn documentation](http://scikit-learn.org/stable/)\n",
    "\n",
    "__License__\n",
    "\n",
    "This work is licensed under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "\n",
    "1. Turn the pseudo-code for the A-priori algorithm from the lecture into a quick-and-dirty implementation. (Efficiency is not crucial, because the dataset is not that large. Correctness is. An implementation with python lists should suffice, see the end of the notebook.)\n",
    "2. Use your algorithm from 1. on the dataset to find frequent itemsets. \n",
    "Note: The algorithm from the lecture runs only on binary features; So, you have to turn the numerical features into binaries, e.g. by comparison with the average.\n",
    "Note: This requires the choice of a suitable minimum support; in practice you don't know \"suitable\" in advance, so binary search the possible values for sigma until the number of frequent itemsets is easy for you to examine by hand.\n",
    "3. Bonus: Use the frequent itemsets of 2. to find strong association rules. Note: Analogously to 2., this requires the choice of a proper confidence. What is the best possible confidence that you can achieve?\n",
    "4. Bonus: Filter your association rules for correlation rules, e.g. using lift as correlation measure.\n",
    "5. Interpretation: Could the observed rule be an effect of a larger cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Implementation of the Apriori-Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example from the Lecture, slide 8\n",
    "names = ['milk', 'butter', 'coffee', 'cacao', 'cake', 'sugar', 'tea']\n",
    "X = [[1, 1, 0, 0, 0, 0, 0],\n",
    "     [1, 0, 1, 0, 1, 0, 0],\n",
    "     [1, 0, 0, 1, 1, 0, 0],\n",
    "     [0, 0, 1, 0, 0, 1, 1],\n",
    "     [1, 0, 1, 0, 0, 1, 0],\n",
    "     [0, 0, 0, 0, 0, 1, 1]]\n",
    "# Some 2-itemsets\n",
    "I1 = [0, 1]\n",
    "I2 = [1, 5]\n",
    "I3 = [0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X[1]\n",
    "any([x[j] == 1 for j in I1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 0, 0, 0, 0, 0]]\n",
      "[]\n",
      "[[1, 0, 1, 0, 1, 0, 0], [1, 0, 1, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Return corresponding instance sets\n",
    "def is_instance(x, I):\n",
    "    return all([x[j] == 1 for j in I])\n",
    "\n",
    "def all_instances(X, I):\n",
    "    return [x for x in X if is_instance(x, I)]\n",
    "\n",
    "print(all_instances(X, I1))\n",
    "print(all_instances(X, I2))\n",
    "print(all_instances(X, I3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666666\n",
      "0.0\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "def support(X, I):\n",
    "    return len(all_instances(X, I))/len(X)\n",
    "print(support(X, I1))\n",
    "print(support(X, I2))\n",
    "print(support(X, I3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[[1, 1, 0, 0, 0, 0, 0]]\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def is_frequent_itemset(X, L, sigma):\n",
    "    return all([support(X, I) >= sigma for I in L])\n",
    "\n",
    "print(I1)\n",
    "print(all_instances(X, I1))\n",
    "print(is_frequent_itemset(X, all_instances(X, I1), 1/6))\n",
    "print(is_frequent_itemset(X, all_instances(X, I1), 1/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [2], [4], [5], [6], [0, 2], [0, 4], [2, 5], [5, 6]]\n",
      "[[0], [2], [5]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def hasAprioriProperty(c, Llast):\n",
    "    for item in c:\n",
    "        d = c[:]\n",
    "        d.remove(item)\n",
    "        if d not in Llast:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def aPriori(X, sigma, verbose=False):\n",
    "    L1 = [[i] for i in range(len(X[0])) if support(X, [i]) >= sigma]\n",
    "    L = [L1]\n",
    "    for k in range(2, len(X) + 1):\n",
    "        if verbose:\n",
    "            print('start loop with k=%d and L=%s' %(k, L))\n",
    "        C = [sorted(a + b) for a in L[-1] for b in L1 if b[0] not in a]\n",
    "        C = [c for i, c in enumerate(C) if c not in C[:i]]    # remove duplicates, because we use lists instead of sets\n",
    "        if verbose:\n",
    "            print('before Apriori-Check C=%s' %C)\n",
    "        C = [c for c in C if hasAprioriProperty(c, L[-1])]\n",
    "        if verbose:\n",
    "            print('after Apriori-Check C=%s' %C)\n",
    "        Csigma = dict([(tuple(c), 0.0) for c in C])\n",
    "        for x in X:\n",
    "            Ct = [c for c in C if is_instance(x, c)]\n",
    "            for c in Ct:\n",
    "                Csigma[tuple(c)] += 1/len(X)\n",
    "        C = [c for c in C if Csigma[tuple(c)] >= sigma]\n",
    "        if len(C) == 0:\n",
    "            break\n",
    "        L += [C]       \n",
    "    return [c for Lk in L for c in Lk ]\n",
    "\n",
    "print(aPriori(X, 0.3))\n",
    "print(aPriori(X, 0.5))\n",
    "print(aPriori(X, 0.7))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [2], [3], [4], [0, 1], [0, 2], [0, 4], [1, 2], [1, 3], [1, 4], [0, 1, 2], [0, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "# Example from Lecture p. 21\n",
    "names = ['I1', 'I2', 'I3', 'I4', 'I5']\n",
    "X     = [[1,    1,    0,    0,    1],\n",
    "         [0,    1,    0,    1,    0],\n",
    "         [0,    1,    1,    0,    0],\n",
    "         [1,    1,    0,    1,    0],\n",
    "         [1,    0,    1,    0,    0],\n",
    "         [0,    1,    1,    0,    0],\n",
    "         [1,    0,    1,    0,    0],\n",
    "         [1,    1,    1,    0,    1],\n",
    "         [1,    1,    1,    0,    0]]\n",
    "\n",
    "print(aPriori(X, 2/9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent itemset mining on Boston Housing Data\n",
    "\n",
    "We'll do frequent itemset mining on the Boston Housing Data. Most of the features are numerical and we'll turn them into Boolean by splitting across the mean/median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "experiment = load_boston()\n",
    "data, target, feat = experiment.data, experiment.target, experiment.feature_names\n",
    "print(experiment['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.32000000e-03   1.80000000e+01   2.31000000e+00]\n",
      " [  2.73100000e-02   0.00000000e+00   7.07000000e+00]\n",
      " [  2.72900000e-02   0.00000000e+00   7.07000000e+00]\n",
      " [  3.23700000e-02   0.00000000e+00   2.18000000e+00]\n",
      " [  6.90500000e-02   0.00000000e+00   2.18000000e+00]]\n",
      "[  3.59376071  11.36363636  11.13677866]\n",
      "[[False  True False]\n",
      " [False False False]\n",
      " [False False False]\n",
      " [False False False]\n",
      " [False False False]]\n",
      "['CRIM' 'ZN' 'INDUS']\n"
     ]
    }
   ],
   "source": [
    "# join data and target for unsupervised learning\n",
    "target = target.reshape(506, 1)\n",
    "Xfull = np.concatenate((data, target), axis=1)\n",
    "np.append(feat, 'MEDV')\n",
    "\n",
    "print(Xfull[:5, :3])\n",
    "\n",
    "# turn it into a boolean dataset by comparison with the \"average\"\n",
    "avg = Xfull.mean(axis=0)\n",
    "# from numpy import median\n",
    "# avg = median(X, axis=0)\n",
    "X = Xfull > avg\n",
    "\n",
    "print(avg[:3])\n",
    "print(X[:5, :3])\n",
    "print(feat[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2], [4], [5], [6], [7], [10], [11], [12], [13], [6, 11], [10, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(aPriori(X, .4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "78px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
