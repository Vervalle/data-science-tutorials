{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Selection and Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a good estimator?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class= \"alert alert-warning\">\n",
    "Model Selection refers to the process of tuning parameters and hyper-parameters in order to find a good machine learning estimator. \n",
    "</div>\n",
    "\n",
    "It requires the \n",
    "\n",
    "- Selection of an error measure\n",
    "- Definition of a test procedure that measures the generalisation performance over unseen examples\n",
    "\n",
    "\n",
    "## Error Measures\n",
    "\n",
    "### Accuracy\n",
    "The most basic measure for a binary classification task is the accuracy, i.e.\n",
    "\n",
    "$$\n",
    " accuracy =\\frac{\\sum_{x_i}y_i == h(x_i)}{|X|}\n",
    "$$\n",
    "\n",
    "where $h(x_i)$ is the estimator for example $x_i$ and $y_i$ is the true label of $x_i$\n",
    "\n",
    "\n",
    "`sklearn` provides a `score` method that calculates this error measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#svc.score(iris_X_test,iris_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is often not an ideal measure, due to several reasons\n",
    "\n",
    "- For unbalanced data sets, i.e. more negative (positive) examples than positive (negative) examples, it does not represent the real performance of an estimator.\n",
    "- For multilabel data, it is unclear how to evaluate the equality of the estimator and the true label\n",
    "- It does not provide any insights on whether the estimator makes errors du to accepting too many examples to a concept, or rejecting too many of them.\n",
    "\n",
    "### Precision and Recall\n",
    "\n",
    "Precision and Recall are two measures coming from Information Retrieval, which give insights on the quality of error. They are build on the concept of a contingence table. \n",
    "\n",
    "**Precision** defines the **accuracy** of an estimator **wrt. a certain classification decision**\n",
    "\n",
    "**Recall** defines the **completeness** of an estimator **wrt. a certain classification decision**\n",
    "\n",
    "To illustrate that we introduce the concept of a contingence table. \n",
    "\n",
    "### Contingence Table\n",
    "\n",
    "\n",
    "**Formal definitions**\n",
    "\n",
    "- estimator $h(x)$\n",
    "- examples $X={<x_1,Y_1>,\\ldots,<x_n,Y_n>}$, with $Y_n \\subset Y$ and $Y$ being the **set of possible classes** an example \n",
    "\n",
    "For every class $a\\in Y$ we define the contingency table as:\n",
    "\n",
    "|Contintency Table for Class $a$| $y_i == a$ | $y_i \\neq a$|\n",
    "|---|---|---|\n",
    "|$h(x_i)==a$|True Positive (TP)|False Positive (FP)|\n",
    "|$h(x_i)\\neq a$|False Negative (FN)|True Negative (TN)|\n",
    "\n",
    "- `{False|True}` reflects whether the outcome was correct or not\n",
    "- `{Positive|Negative}` reflects whether the decision was to assign it to $a$ (positive) or not (negative)\n",
    "\n",
    "\n",
    "\n",
    "**Precision** now estimates the probability that an assignment made by $h$ is correct:\n",
    "\n",
    "$$\n",
    "Precision_a = \\frac{TP_a}{TP_a+FP_a}\n",
    "$$\n",
    "\n",
    "**Recall** now estimates the probability that $h$ has found all correct assignments\n",
    "\n",
    "$$\n",
    "Recall_a = \\frac{TP_a}{TP_a+FN_a}\n",
    "$$\n",
    "\n",
    "**F1** defines a combined measure, which is the harmonic mean between Precision and Recall:\n",
    "\n",
    "$$\n",
    "F1_a = \\frac{2*Precision_a*Recall_a}{Precision_a + Recall_a}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Microaveraging vs. Macroaveraging\n",
    "\n",
    "Precision and Recall define the quality estimators with respect to a specific class $a$.\n",
    "\n",
    "Given multiple classes $a\\in Y$, there are two ways for generating a single estimator:\n",
    "\n",
    "- **Microaveraging** sums over all single decisions, i.e. \n",
    "$$\n",
    "Precision^\\mu = \\frac{\\sum_{a\\in Y}TP_a}{\\sum_{a\\in Y}TP_a+\\sum_{a\\in Y}FP_a}\n",
    "$$\n",
    "- **Macroaveraging** sums over all class estimators, i.e.\n",
    "\n",
    "$$\n",
    "Precision^\\nu = \\frac{1}{|Y|}\\sum_{a\\in Y}Precision_a\n",
    "$$\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Note that in the case of microaveraging in a single label classification:\n",
    "$Precision^\\mu==Recall^\\mu== Accuracy$.\n",
    "\n",
    "The reason is, that every FN becomes a FP in another class and hence the sum of FN and FP is the same.\n",
    "</div>\n",
    "\n",
    "\n",
    "Further details can be found under [1] and [2]\n",
    "\n",
    "[1] http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\n",
    "\n",
    "[2] http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html\n",
    "\n",
    "\n",
    "## Test Procedure and Parameter Estimation\n",
    "\n",
    "\n",
    "In the examples above we have always used a fraction of the example for the training and the remaining examples for testing.\n",
    "\n",
    "- Why?\n",
    "- What is the impact of a random split?\n",
    "- Can we do better?\n",
    "\n",
    "\n",
    "\n",
    "Note that we want to generalize over our given data. So we need to split the data into a training and a test set to get an unbiased estimator of the model quality on **unseen examples**. \n",
    "\n",
    "A **random split** can be very bad or very good. So we would need more random splits to avoid such a bias.\n",
    "\n",
    "A better, more efficient method is called **cross-validation**.\n",
    "\n",
    "\n",
    "### k-folded Cross Validation\n",
    "\n",
    "Given our examples $X$, we split them into $k-folds$ of approximate equal size. \n",
    "\n",
    "1. for every fold $i$ in $k we\n",
    "\n",
    "    1. Fit our model on all but the examples in the $k^{th}$ fold\n",
    "    2. Estimate the quality of our model on the examples from the  $k^{th}$ fold\n",
    "    \n",
    "2. Average the obtained quality indicators (e.g. precision/recall) over al the $k-folds$\n",
    "\n",
    "\n",
    "<p>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "When averaging over quality indicators like accuracy, always investigate the standard deviation in addition to the average value. The standard deviation tells you how robust your estimated quality indicator is wrt. different runs.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [2 3 4 5] | test: [0 1]\n",
      "Train: [0 1 4 5] | test: [2 3]\n",
      "Train: [0 1 2 3] | test: [4 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#sklearn provides cv functionality as generator function\n",
    "from sklearn import cross_validation\n",
    "k_fold = cross_validation.KFold(n=6, n_folds=3)\n",
    "for train_indices, test_indices in k_fold:\n",
    "     print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation on the iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.000000\n",
      "Score: 1.000000\n",
      "Score: 1.000000\n",
      "Score: 0.947368\n",
      "Score: 0.947368\n",
      "Score: 0.947368\n",
      "Score: 0.944444\n",
      "Score: 0.888889\n",
      "Average model accuracy 0.959430 +/- 0.036357\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "k_fold = cross_validation.KFold(n=iris.data.shape[0], n_folds=8)\n",
    "\n",
    "scores = []\n",
    "for train_indices, test_indices in k_fold:\n",
    "     logistic = linear_model.LogisticRegression(C=1e5)\n",
    "     logistic.fit(iris.data[train_indices], iris.target[train_indices])  \n",
    "     scores.append(\n",
    "                  logistic.score(iris.data[test_indices], \n",
    "                                 iris.target[test_indices])\n",
    "                  )\n",
    "     print ('Score: %f' % (scores[-1]))\n",
    "\n",
    "print (\"Average model accuracy %f +/- %f\" % (np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Questions**\n",
    "\n",
    "- Run the above code with $n=3$. What is wrong and why?\n",
    "- Research the concept of stratified cross validation\n",
    "- What is the highest possible $k$?\n",
    "- Research the concepts of leave-one-out testing and leave-one-label-out testing\n",
    "\n",
    "Try to answer them when working on the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Text Classification with scikit-learn\n",
    "Fetch the 20 Newsgroups data set and load it via the  `sklearn.datasets.fetch_20newsgroups` function. The data set contains 20 newsgroups with 1000 posts in each newsgroup. The data set comes with a test/training split. \n",
    "\n",
    "Conduct the following tasks:\n",
    "1. Preprocess the data set using scikit learn vectorizer\n",
    "2. Train the data set using two classifiers of your choice\n",
    "3. Tune the parameters of those classifiers to obtain the best results\n",
    "4. Evaluate the performance on the test set and compare both classifiers.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "The solution can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "# References and Further Reading\n",
    "\n",
    "- Tom Mitchell, Machine Learning, McGraw-Hill 1997 [chapter slides](http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html)\n",
    "- [Scikit learn](http://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "244px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
