{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning in a Nutshell with scikit-learn\n",
    "    \n",
    "## Overview and Preprocessing\n",
    "\n",
    "\n",
    "by \n",
    "\n",
    "[__Michael Granitzer__ (michael.granitzer@uni-passau.de)]( http://www.mendeley.com/profiles/michael-granitzer/)\n",
    "and [Konstantin Ziegler (konstantin.ziegler@uni-passau.de)](http://zieglerk.net)\n",
    "\n",
    "with examples taken from the scikit-learn documentation under http://scikit-learn.org/stable/\n",
    "\n",
    "\n",
    "__License__\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution 3.0 Unported License](http://creativecommons.org/licenses/by/3.0/) (CC BY 3.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "0. What is scikit-learn (sklearn)\n",
    "1. Scikit Learning and Machine Learning Overview\n",
    "2. Data Preprocessing\n",
    "\n",
    "\n",
    "You may also refer to the [scikit-learning Introduction](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_number": 3
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "scikit-learn is a Machine Learning library in Python ([Homepage](http://scikit-learn.org/stable/)).\n",
    "\n",
    "\n",
    "* Simple and efficient tools for data mining and data analysis\n",
    "* Accessible to everybody, and reusable in various contexts\n",
    "* Built on NumPy, SciPy, and matplotlib\n",
    "* Open source, commercially usable - BSD license\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 4,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 9,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 14,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Refers to the task to create and prepare the data to be consumed by the learning algorithm. Usually, the target format is an matrix holding the preprocessed data. Sklearn uses numpy for representing data.\n",
    "\n",
    "\n",
    "Preprocessing steps can be summarized as follows:\n",
    "\n",
    "1. **Feature Extraction/Integration**: Convert data into matrix or integrate different data sources into one matrix\n",
    "2. **Feature Manipulation**: Manipulate and reorganise the features of a matrix\n",
    "    * *Feature Weighting/Scaling*: Convert the range of feature values\n",
    "    * *Feature Selection*: Removing unnecessary or low quality features\n",
    "    * *Feature Transformation (Dimensionality Reduction)*: merge or combine existing features to create new features   \n",
    "    \n",
    "3. **Dataset Manipulation**: Manipulate/eliminate data points\n",
    "    * *Subsampling*: Reduce the amount of data points in case the data set is to large (Squashing)\n",
    "    * *Outlier Detection*: Remove data sets that do not fit to the data distribution\n",
    "             \n",
    "<p>\n",
    "<div class=\"alert alert-info\">\n",
    "**Feature Engineering**, the task of creating features from real world data, is the most important and time consuming step (when you apply machine learning techniques)\n",
    "</div>\n",
    "\n",
    "See http://scikit-learn.org/stable/data_transforms.html for details on preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 14,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loading/Creating standard data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 14,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "sklearn offers functions for \n",
    "- loading pre-packaged data sets\n",
    "- importing data sets from other sources (in various formats, in particular `svm_light`)\n",
    "- generating your own artifical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 14,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load Pre-Packaged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 14
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as ds\n",
    "boston = ds.load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 14,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (506, 13)\n",
      "Features: ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "Labels: (506,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Shape:\", boston.data.shape)\n",
    "print(\"Features:\", boston.feature_names)\n",
    "print(\"Labels:\", boston.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 14,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Imort Dataset in SVM-Light Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 21,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The SVM-Light Data Format is a defacto standard for storing and loading sparse data matrices in a text format. \n",
    "\n",
    "The format is\n",
    "\n",
    "    Label attribute_num:attribute_value attribute_num:attribute_value ..\n",
    "    Label attribute_num:attribute_value attribute_num:attribute_value ..\n",
    "    \n",
    "where every data points is one line\n",
    "\n",
    "\n",
    "\n",
    "- Data sets in that format can be found under http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\n",
    "- A faster API compatible implementation can be found under  https://github.com/mblondel/svmlight-loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 21,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-1 3:1 11:1 14:1 19:1 39:1 42:1 55:1 64:1 67:1 73:1 75:1 76:1 80:1 83:1 \\n-1 3:1 6:1 17:1 27:1 35:1 40:1 57:1 63:1 69:1 73:1 '\n"
     ]
    }
   ],
   "source": [
    "import requests as r\n",
    "data = r.get(\"http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a\")\n",
    "print(data.content[0:124])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 23,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (1605, 119)\n",
      "<class 'numpy.ndarray'> (1605,)\n"
     ]
    }
   ],
   "source": [
    "data = r.get(\"http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a\"\n",
    "             ,stream = True)\n",
    "X_train, y_train = ds.load_svmlight_file(data.raw)\n",
    "print(type(X_train), X_train.shape)\n",
    "print(type(y_train), y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 23,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generate Artificial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 25,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (100, 10)\n",
      "<class 'numpy.ndarray'> (100,)\n"
     ]
    }
   ],
   "source": [
    "#see the sklearn.dataset moduls with prefix make\n",
    "# make_classificaiton for example creates a n-class classification problem\n",
    "X_train, y_train = ds.make_classification(100,10)\n",
    "\n",
    "print(type(X_train), X_train.shape)\n",
    "print(type(y_train), y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 25,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 25,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class =\"alert alert-warning\">\n",
    "**Feature Extraction** refers to the process of extracting features out of  the raw data an represent those features in a formal, re-usable model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 25,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extracting Features from Dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 29
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "sklearn allows you to convert python dictionaries, that represent features, into Numpy arrays.\n",
    "\n",
    "For nominal data it implements a \"one-hot\" coding (e..g one Attribute that is on or off)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 30,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.00000000e+04   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    1.32000000e+02   5.00000000e+03]\n",
      " [  1.00000000e+04   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    3.32000000e+02   5.00000000e+04]\n",
      " [  8.00000000e+04   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "    9.00000000e+01   1.50000000e+04]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mileage',\n",
       " 'Model=Porsche Carrera',\n",
       " 'Model=Renault Scienic',\n",
       " 'Model=Touran',\n",
       " 'Power',\n",
       " 'Price']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurements = [\n",
    "        {'Model': 'Renault Scienic', 'Mileage': 50000, 'Power': 132, 'Price':5000},\n",
    "        {'Model': 'Porsche Carrera', 'Mileage': 10000, 'Power': 332, 'Price':50000},\n",
    "        {'Model': 'Touran', 'Mileage': 80000, 'Power': 90, 'Price': 15000}\n",
    "        ]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "\n",
    "print(vec.fit_transform(measurements).toarray())\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 30,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text Preprocessing using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 32
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "sklearn supports several counting methods for converting text into a matrix representation. The simplest one is the count vectorizer.\n",
    "\n",
    "Vectorizers can use analyzers (to be set in the constructor), which tokenize the text. Here you can integrate tokenizers from other libraries, like for example NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 33,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "corpus = ['This is the first document.',\n",
    "        'This is the second second document.',\n",
    "        'And the third one.',\n",
    "        'Is this the first document?']\n",
    "word_counts = vectorizer.fit_transform(corpus)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 33,
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 33,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Scaling and Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 36,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After extracting features one needs to consider the scale and/or value of a feature. Most likely, value ranges are not sufficiently prepared for subsequent machine learning. \n",
    "\n",
    "For example, raw counts of feature occurence may not provide a meaningful feature representation. In text for example, the words with the highest frequency are stopwords and hence we need to reweight the value of a feature. \n",
    "\n",
    "\n",
    "As a second example, data coming from a sensor might contain wrong measurements or the scale between two sensors might be wrong and needs to be rescaled/normalized. \n",
    "<p>\n",
    "\n",
    "<div class = \"alert alert-info\">\n",
    "When preprocessing data, always check that \n",
    "<ol>\n",
    "<li> The extracted features (i.e. the attributes/dimensions) are meaningful and represent information such that the learning task can be solved \n",
    "  <li> The value range of the features is as expected by the machine learning algorithm and has been cleaned from problematic data\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 36,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TFIDF Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 38
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "TF IDF weighting stands for \"Term Frequency vs. Inverse Document Frequency\" weighting and is mostly used for representing textual data.\n",
    "\n",
    "The weight is proportional to the frequency how often a word occurs in a text multiplied by the inverse document frequency, i.e. how many documents contain a certain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 39,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf_counts = transformer.fit_transform(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 39,
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document\tfeature\ttfidf\tcount\n",
      "0\tand\t\t0.000000\t\t\t\t0\n",
      "0\tdocument\t\t0.438777\t\t\t\t1\n",
      "0\tfirst\t\t0.541977\t\t\t\t1\n",
      "0\tis\t\t0.438777\t\t\t\t1\n",
      "0\tone\t\t0.000000\t\t\t\t0\n",
      "0\tsecond\t\t0.000000\t\t\t\t0\n",
      "0\tthe\t\t0.358729\t\t\t\t1\n",
      "0\tthird\t\t0.000000\t\t\t\t0\n",
      "0\tthis\t\t0.438777\t\t\t\t1\n",
      "1\tand\t\t0.000000\t\t\t\t0\n",
      "1\tdocument\t\t0.272301\t\t\t\t1\n",
      "1\tfirst\t\t0.000000\t\t\t\t0\n",
      "1\tis\t\t0.272301\t\t\t\t1\n",
      "1\tone\t\t0.000000\t\t\t\t0\n",
      "1\tsecond\t\t0.853226\t\t\t\t2\n",
      "1\tthe\t\t0.222624\t\t\t\t1\n",
      "1\tthird\t\t0.000000\t\t\t\t0\n",
      "1\tthis\t\t0.272301\t\t\t\t1\n",
      "2\tand\t\t0.552805\t\t\t\t1\n",
      "2\tdocument\t\t0.000000\t\t\t\t0\n",
      "2\tfirst\t\t0.000000\t\t\t\t0\n",
      "2\tis\t\t0.000000\t\t\t\t0\n",
      "2\tone\t\t0.552805\t\t\t\t1\n",
      "2\tsecond\t\t0.000000\t\t\t\t0\n",
      "2\tthe\t\t0.288477\t\t\t\t1\n",
      "2\tthird\t\t0.552805\t\t\t\t1\n",
      "2\tthis\t\t0.000000\t\t\t\t0\n",
      "3\tand\t\t0.000000\t\t\t\t0\n",
      "3\tdocument\t\t0.438777\t\t\t\t1\n",
      "3\tfirst\t\t0.541977\t\t\t\t1\n",
      "3\tis\t\t0.438777\t\t\t\t1\n",
      "3\tone\t\t0.000000\t\t\t\t0\n",
      "3\tsecond\t\t0.000000\t\t\t\t0\n",
      "3\tthe\t\t0.358729\t\t\t\t1\n",
      "3\tthird\t\t0.000000\t\t\t\t0\n",
      "3\tthis\t\t0.438777\t\t\t\t1\n"
     ]
    }
   ],
   "source": [
    "print(\"Document\\tfeature\\ttfidf\\tcount\")\n",
    "for row in range(tfidf_counts.shape[0]):\n",
    "    for col,name in enumerate(vectorizer.get_feature_names()):\n",
    "        print(\"%d\\t%s\\t\\t%f\\t\\t\\t\\t%d\"%\\\n",
    "              (row,name,\n",
    "               tfidf_counts[row,col],\n",
    "               word_counts[row,col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 39,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Standardization (mean removal/variance scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 42,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some machine learning methods do not work well if the value range of attributes is not standardized. Standardization assume that values are normally distributed and aims at removing mean and scaling the values to unit variance.\n",
    "\n",
    "Standardization is often refered to as Feature Normalization (i.e. normalization along one attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 42,
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean original data [ 1.          0.          0.33333333]\n",
      "Var  original data [ 0.66666667  0.66666667  1.55555556]\n",
      "Mean scaled   data [ 0.  0.  0.]\n",
      "Var  scaled   data [ 1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([[ 1., -1.,  2.],\n",
    "               [ 2.,  0.,  0.],\n",
    "               [ 0.,  1., -1.]])\n",
    "X_scaled = preprocessing.scale(X)\n",
    "print(\"Mean original data\", np.mean(X, axis=0))\n",
    "print(\"Var  original data\",np.var(X, axis=0))\n",
    "print(\"Mean scaled   data\",np.mean(X_scaled, axis=0))\n",
    "print(\"Var  scaled   data\",np.var(X_scaled, axis=0))\n",
    "                                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 42,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Min/Max Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 45,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively, one can simply scale the feature range according to the minimum and maximum value in the data set such that the new feature range is in the range $[0:1]$. \n",
    "\n",
    "This is done by the `MinMaxScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 45,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5         0.          1.        ]\n",
      " [ 1.          0.5         0.33333333]\n",
      " [ 0.          1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale_minmax = min_max_scaler.fit_transform(X)\n",
    "print(X_scale_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 45
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -1.  2.]\n",
      " [ 2.  0.  0.]\n",
      " [ 0.  1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 45,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#alternative calculation\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) \n",
    "print(X_std - X_scale_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 45,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 50,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Normalization deals with the problem that data point vectors can be of very different length. Consider for example a short and a long document.\n",
    "\n",
    "Normalization brings all data points to unit length. This is necessary by methods relying on the dot product between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 50,
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.44948974  2.          1.41421356]\n",
      "[ 1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "print(np.linalg.norm(X, ord=2, axis=1))\n",
    "print(np.linalg.norm(X_normalized, ord=2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 50,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Final Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 53,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class = \"alert alert-info\">\n",
    "In practical applications, **Preprocessing** is the most cruical step in applying machine learning. It depends on the machine learning technique used afterwards, the data at hand and the skill of the feature engineer.\n",
    "<p>\n",
    "So do not underestimate this step. A good to ask is whether you, as a human, could solve the task given the information obtained from preprocessing. If you can't, the machine will, most likely, can't do it either.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 53,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 55
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Tom Mitchell, Machine Learning, McGraw-Hill 1997 [chapter slides](http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html)\n",
    "- [Scikit learn](http://scikit-learn.org/stable/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "375px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
