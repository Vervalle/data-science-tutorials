{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview and Preprocessing\n",
    "## Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Lets assume we have collected a data set about cars:\n",
    "\n",
    "|Customer Group| Model | Mileage | Power | Price |\n",
    "|-|-|-|-|-|\n",
    "|Family| Renault Scenic | 50,000 | 132 | 5,000|\n",
    "|Upper Class | Porsche Carrera | 10,000 | 332 | 50,000|\n",
    "|Family | Touran  | 80,000 | 90 | 15,000|\n",
    "| ... | ... | ... | ... | ... |\n",
    "|?| Wonder Car| 500 | 4000 | ?|\n",
    "\n",
    "- Given a large set of cars, can we group together cars with similar price, power and mileage?\n",
    "- Can we predict the price of a new car given mileage and power?\n",
    "- Can we predict the customer group?\n",
    "- What kind of cars do upper class people drive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Approach\n",
    "\n",
    "The usual machine learning setup is:\n",
    "\n",
    "1. **$n$ data samples** (e.g. $n$ cars), representing the past experience\n",
    "2. Every data sample is described by a **set of d features/attributes** (e.g. horsepower and price of the car)\n",
    "\n",
    "|$Attribut_1$|$Attribut_2$|$\\ldots$|$Attribut_d$|\n",
    "|-|-|-|-|\n",
    "|$Attribut_1$ of $Example_1$|$Attribut_2$ of $Example_1$ |$\\ldots$|$Attribut_d$ of $Example_1$|\n",
    "|$Attribut_1$ of $Example_2$|$Attribut_2$ of $Example_2$ |$\\ldots$|$Attribut_d$ of $Example_2$|\n",
    "|$\\ldots$|$\\ldots$|$\\ldots$|$\\ldots$|\n",
    "|$Attribut_d$ of $Example_n$|$Attribut_2$ of $Example_n$ |$\\ldots$|$Attribut_d$ of $Example_n$|\n",
    "\n",
    "Machine learning estimates a **model** (also called hypothesis) that **'best' fits the data**. Fitting means the model\n",
    "\n",
    "1. **predicts** features of yet unkown examples (e.g. predict the customer group of a car)\n",
    "2. **describes** properties of the examples (e.g. points belonging together)\n",
    "\n",
    "Building such a model is called learning, training or model fitting.\n",
    "\n",
    "Using such a model is often call \"testing\", \"model estimation\" or \"inference step\".\n",
    "\n",
    "Converting data into the necessary format for learning and testing is called **preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn\n",
    "\n",
    "scikit-learn is a Machine Learning library in Python ([Homepage](http://scikit-learn.org/stable/)).\n",
    "\n",
    "\n",
    "* Simple and efficient tools for data mining and data analysis\n",
    "* Accessible to everybody, and reusable in various contexts\n",
    "* Built on NumPy, SciPy, and matplotlib\n",
    "* Open source, commercially usable - BSD license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Refers to the task to create and prepare the data to be consumed by the learning algorithm. Usually, the target format is a matrix holding the preprocessed data. Sklearn uses numpy for representing data.\n",
    "\n",
    "\n",
    "Preprocessing steps can be summarized as follows:\n",
    "\n",
    "1. **Feature Extraction/Integration**: Convert data into matrix or integrate different data sources into one matrix\n",
    "2. **Feature Manipulation**: Manipulate and reorganise the features of a matrix\n",
    "    * *Feature Weighting/Scaling*: Convert the range of feature values\n",
    "    * *Feature Selection*: Removing unnecessary or low quality features\n",
    "    * *Feature Transformation (Dimensionality Reduction)*: merge or combine existing features to create new features   \n",
    "    \n",
    "3. **Dataset Manipulation**: Manipulate/eliminate data points\n",
    "    * *Subsampling*: Reduce the amount of data points in case the data set is to large (Squashing)\n",
    "    * *Outlier Detection*: Remove data points that do not fit to the data distribution\n",
    "             \n",
    "<p>\n",
    "<div class=\"alert alert-info\">\n",
    "**Feature Engineering**, the task of creating features from real world data, is often the most important and time consuming step (when you apply machine learning techniques)\n",
    "</div>\n",
    "\n",
    "See http://scikit-learn.org/stable/data_transforms.html for details on preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features from Dicts\n",
    "sklearn allows you to convert python dictionaries, that represent features, into Numpy arrays.\n",
    "\n",
    "For nominal data it implements a \"one-hot\" coding (e..g one Attribute that is on or off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.00000000e+04   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    1.32000000e+02   5.00000000e+03]\n",
      " [  1.00000000e+04   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    3.32000000e+02   5.00000000e+04]\n",
      " [  8.00000000e+04   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "    9.00000000e+01   1.50000000e+04]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mileage',\n",
       " 'Model=Porsche Carrera',\n",
       " 'Model=Renault Scenic',\n",
       " 'Model=Touran',\n",
       " 'Power',\n",
       " 'Price']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurements = [\n",
    "        {'Model': 'Renault Scenic', 'Mileage': 50000, 'Power': 132, 'Price':5000},\n",
    "        {'Model': 'Porsche Carrera', 'Mileage': 10000, 'Power': 332, 'Price':50000},\n",
    "        {'Model': 'Touran', 'Mileage': 80000, 'Power': 90, 'Price': 15000}\n",
    "        ]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "\n",
    "print(vec.fit_transform(measurements).toarray())\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing using sklearn\n",
    "sklearn supports several counting methods for converting text into a matrix representation. The simplest one is the count vectorizer.\n",
    "\n",
    "Vectorizers can use analyzers (to be set in the constructor), which tokenize the text. Here you can integrate tokenizers from other libraries, like for example NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 5)\t2\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 8)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 6)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 8)\t1\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "corpus = ['This is the first document.',\n",
    "        'This is the second second document.',\n",
    "        'And the third one.',\n",
    "        'Is this the first document?']\n",
    "word_counts = vectorizer.fit_transform(corpus)\n",
    "print(word_counts)\n",
    "print(vectorizer.get_feature_names())\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling and Weighting\n",
    "After extracting features one needs to consider the scale and/or value of a feature. Most likely, value ranges are not sufficiently prepared for subsequent machine learning. \n",
    "\n",
    "For example, raw counts of feature occurence may not provide a meaningful feature representation. In text for example, the words with the highest frequency are stopwords and hence we need to reweight the value of a feature. \n",
    "\n",
    "\n",
    "As a second example, data coming from a sensor might contain wrong measurements or the scale between two sensors might be wrong and needs to be rescaled/normalized. \n",
    "<p>\n",
    "\n",
    "<div class = \"alert alert-info\">\n",
    "When preprocessing data, always check that \n",
    "<ol>\n",
    "<li> The extracted features (i.e. the attributes/dimensions) are meaningful and represent information such that the learning task can be solved \n",
    "  <li> The value range of the features is as expected by the machine learning algorithm and has been cleaned from problematic data\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Weighting\n",
    "TF IDF weighting stands for \"Term Frequency vs. Inverse Document Frequency\" weighting and is mostly used for representing textual data.\n",
    "\n",
    "The weight is proportional to the frequency how often a word occurs in a text multiplied by the inverse document frequency, i.e. how many documents contain a certain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc\tfeature\t\ttfidf\t\tcount\n",
      "0\tand\t\t0.000000\t0\n",
      "0\tdocument\t\t0.438777\t1\n",
      "0\tfirst\t\t0.541977\t1\n",
      "0\tis\t\t0.438777\t1\n",
      "0\tone\t\t0.000000\t0\n",
      "0\tsecond\t\t0.000000\t0\n",
      "0\tthe\t\t0.358729\t1\n",
      "0\tthird\t\t0.000000\t0\n",
      "0\tthis\t\t0.438777\t1\n",
      "1\tand\t\t0.000000\t0\n",
      "1\tdocument\t\t0.272301\t1\n",
      "1\tfirst\t\t0.000000\t0\n",
      "1\tis\t\t0.272301\t1\n",
      "1\tone\t\t0.000000\t0\n",
      "1\tsecond\t\t0.853226\t2\n",
      "1\tthe\t\t0.222624\t1\n",
      "1\tthird\t\t0.000000\t0\n",
      "1\tthis\t\t0.272301\t1\n",
      "2\tand\t\t0.552805\t1\n",
      "2\tdocument\t\t0.000000\t0\n",
      "2\tfirst\t\t0.000000\t0\n",
      "2\tis\t\t0.000000\t0\n",
      "2\tone\t\t0.552805\t1\n",
      "2\tsecond\t\t0.000000\t0\n",
      "2\tthe\t\t0.288477\t1\n",
      "2\tthird\t\t0.552805\t1\n",
      "2\tthis\t\t0.000000\t0\n",
      "3\tand\t\t0.000000\t0\n",
      "3\tdocument\t\t0.438777\t1\n",
      "3\tfirst\t\t0.541977\t1\n",
      "3\tis\t\t0.438777\t1\n",
      "3\tone\t\t0.000000\t0\n",
      "3\tsecond\t\t0.000000\t0\n",
      "3\tthe\t\t0.358729\t1\n",
      "3\tthird\t\t0.000000\t0\n",
      "3\tthis\t\t0.438777\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf_counts = transformer.fit_transform(word_counts)\n",
    "\n",
    "print(\"Doc\\tfeature\\t\\ttfidf\\t\\tcount\")\n",
    "for row in range(tfidf_counts.shape[0]):\n",
    "    for col,name in enumerate(vectorizer.get_feature_names()):\n",
    "        print(\"%d\\t%s\\t\\t%f\\t%d\"%\\\n",
    "              (row,name,\n",
    "               tfidf_counts[row,col],\n",
    "               word_counts[row,col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization (mean removal/variance scaling)\n",
    "Some machine learning methods do not work well if the value range of attributes is not standardized. Standardization assume that values are normally distributed and aims at removing mean and scaling the values to unit variance.\n",
    "\n",
    "Standardization is often refered to as Feature Normalization (i.e. normalization along one attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean original data [ 1.          0.          0.33333333]\n",
      "Var  original data [ 0.66666667  0.66666667  1.55555556]\n",
      "Mean scaled   data [ 0.  0.  0.]\n",
      "Var  scaled   data [ 1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([[ 1., -1.,  2.],\n",
    "               [ 2.,  0.,  0.],\n",
    "               [ 0.,  1., -1.]])\n",
    "X_scaled = preprocessing.scale(X)\n",
    "print(\"Mean original data\", np.mean(X, axis=0))\n",
    "print(\"Var  original data\",np.var(X, axis=0))\n",
    "print(\"Mean scaled   data\",np.mean(X_scaled, axis=0))\n",
    "print(\"Var  scaled   data\",np.var(X_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Min/Max Scaling\n",
    "Alternatively, one can simply scale the feature range according to the minimum and maximum value in the data set such that the new feature range is in the range $[0:1]$. \n",
    "\n",
    "This is done by the `MinMaxScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5         0.          1.        ]\n",
      " [ 1.          0.5         0.33333333]\n",
      " [ 0.          1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale_minmax = min_max_scaler.fit_transform(X)\n",
    "print(X_scale_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -1.  2.]\n",
      " [ 2.  0.  0.]\n",
      " [ 0.  1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5         0.          1.        ]\n",
      " [ 1.          0.5         0.33333333]\n",
      " [ 0.          1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#alternative calculation\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) \n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "Normalization deals with the problem that data point vectors can be of very different length. Consider for example a short and a long document.\n",
    "\n",
    "Normalization brings all data points to unit length. This is necessary by methods relying on the dot product between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.44948974  2.          1.41421356]\n",
      "[ 1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "print(np.linalg.norm(X, ord=2, axis=1))\n",
    "print(np.linalg.norm(X_normalized, ord=2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Remarks\n",
    "<div class = \"alert alert-info\">\n",
    "In practical applications, **Preprocessing** is the most cruical step in applying machine learning. It depends on the machine learning technique used afterwards, the data at hand and the skill of the feature engineer.\n",
    "<p>\n",
    "So do not underestimate this step. A good to ask is whether you, as a human, could solve the task given the information obtained from preprocessing. If you can't, the machine, most likely, can't do it either.\n",
    "</div>\n",
    "\n",
    "## References\n",
    "- Chapter 1 in Tom Mitchell (1997), Machine Learning, McGraw-Hill. Chapter slides for instructors are [available](http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html)\n",
    "- Tutorial [An introduction to machine learning with scikit-learn](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
