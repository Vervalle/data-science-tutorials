{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Iris Dataset\n",
    "For introducing the different supervised learners we will use the IRIS data set.\n",
    "\n",
    "The iris data set characterises iris-flowers into three sub-groups according to length and width of the outer (petal) and inner (sepal) leaves of a flower.\n",
    "\n",
    "**Sepal vs. Petal**\n",
    "\n",
    "\n",
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Petal-sepal.jpg/220px-Petal-sepal.jpg\"/>\n",
    "\n",
    "**The three classes (setosa, versicolor, virginica)**\n",
    "\n",
    "<div class =\"img-responsive\" style=\"display:inline\" >\n",
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/220px-Kosaciec_szczecinkowaty_Iris_setosa.jpg\" width=\"30%\"/>\n",
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/220px-Iris_versicolor_3.jpg\" width=\"30%\"/>\n",
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/220px-Iris_virginica.jpg\" width=\"30%\"  height=\"10%\" style=\"overflow:hidden\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Images from Wikipedia\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2]\n",
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print (\"Classes:\", np.unique(iris.target))\n",
    "print (iris[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Lets assume we have collected a data set about cars:\n",
    "\n",
    "|Customer Group| Model | Mileage | Power | Price |\n",
    "|-|-|-|-|-|\n",
    "|Family| Renault Scienic | 50,000 | 132 | 5,000|\n",
    "|Upper Class | Porsche Carrera | 10,000 | 332 | 50,000|\n",
    "|Family | Touran  | 80,000 | 90 | 15,000|\n",
    "| ... | ... | ... | ... | ... |\n",
    "|?| Wonder Car| 500 | 4000 | ?|\n",
    "\n",
    "- Given a large set of cars, can we group together cars with similar price, power and mileage?\n",
    "- Can we predict the price of a new car given mileage and power?\n",
    "- Can we predict the customer group?\n",
    "- What kind of cars do upper class people drive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real World Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Image Analysis: Face Recognition, Object Detection, Self-driving Cars\n",
    "- Text Analysis: POS Tagging, Named Entity Recognition, Speech-to-Text Analysis\n",
    "- Robotics\n",
    "- E-Commerce\n",
    "- Information Retrieval and Recommender Systems\n",
    "- $\\ldots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual machine learning setup is:\n",
    "\n",
    "1. **$n$ data samples** (e.g. $n$ cars), representing the past experience\n",
    "2. Every data sample is described by a **set of d features/attributes** (e.g. horsepower and price of the car)\n",
    "\n",
    "|$Attribut_1$|$Attribut_2$|$\\ldots$|$Attribut_d$|\n",
    "|-|-|-|-|\n",
    "|$Attribut_1$ of $Example_1$|$Attribut_2$ of $Example_1$ |$\\ldots$|$Attribut_d$ of $Example_1$|\n",
    "|$Attribut_1$ of $Example_2$|$Attribut_2$ of $Example_2$ |$\\ldots$|$Attribut_d$ of $Example_2$|\n",
    "|$\\ldots$|$\\ldots$|$\\ldots$|$\\ldots$|\n",
    "|$Attribut_d$ of $Example_n$|$Attribut_2$ of $Example_n$ |$\\ldots$|$Attribut_d$ of $Example_n$|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning estimates a **model** (also called hypothesis) that **'best' fits the data**. Fitting means the model\n",
    "\n",
    "1. **predicts** features of yet unkown examples (e.g. predict the class a flower belongs to)\n",
    "2. **describes** properties of the examples (e.g. points belonging togehter)\n",
    "\n",
    "Building such a model is called learning, training or model fitting.\n",
    "\n",
    "Using such a model is often call \"testing\", \"model estimation\" or \"inference step\".\n",
    "\n",
    "Converting data into the necessary format for learning and testing is called **preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refers to the task to create and prepare the data to be consumed by the learning algorithm. Usually, the target format is an matrix holding the preprocessed data. Sklearn uses numpy for representing data.\n",
    "\n",
    "\n",
    "Preprocessing steps can be summarized as follows:\n",
    "\n",
    "1. **Feature Extraction/Integration**: Convert data into matrix or integrate different data sources into one matrix\n",
    "2. **Feature Manipulation**: Manipulate and reorganise the features of a matrix\n",
    "    * *Feature Weighting/Scaling*: Convert the range of feature values\n",
    "    * *Feature Selection*: Removing unnecessary or low quality features\n",
    "    * *Feature Transformation (Dimensionality Reduction)*: merge or combine existing features to create new features   \n",
    "    \n",
    "3. **Dataset Manipulation**: Manipulate/eliminate data points\n",
    "    * *Subsampling*: Reduce the amount of data points in case the data set is to large (Squashing)\n",
    "    * *Outlier Detection*: Remove data sets that do not fit to the data distribution\n",
    "             \n",
    "<p>\n",
    "<div class=\"alert alert-info\">\n",
    "**Feature Engineering**, the task of creating features from real world data, is the most important and time consuming step (when you apply machine learning techniques)\n",
    "</div>\n",
    "\n",
    "See http://scikit-learn.org/stable/data_transforms.html for details on preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading/Creating standard data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn offers functions for \n",
    "- loading pre-packaged data sets\n",
    "- importing data sets from other sources (in various formats, in particular `svm_light`)\n",
    "- generating your own artifical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Packaged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as ds\n",
    "boston = ds.load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (506, 13)\n",
      "Features: ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "Labels: (506,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Shape:\", boston.data.shape)\n",
    "print(\"Features:\", boston.feature_names)\n",
    "print(\"Labels:\", boston.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## UCI CAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘CAR’: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 51867  100 51867    0     0  52075      0 --:--:-- --:--:-- --:--:-- 56377\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3097  100  3097    0     0   6149      0 --:--:-- --:--:-- --:--:--  6206\n",
      "1. Title: Car Evaluation Database\n",
      "\n",
      "2. Sources:\n",
      "   (a) Creator: Marko Bohanec\n",
      "   (b) Donors: Marko Bohanec   (marko.bohanec@ijs.si)\n",
      "               Blaz Zupan      (blaz.zupan@ijs.si)\n",
      "   (c) Date: June, 1997\n",
      "\n",
      "3. Past Usage:\n",
      "\n",
      "   The hierarchical decision model, from which this dataset is\n",
      "   derived, was first presented in \n",
      "\n",
      "   M. Bohanec and V. Rajkovic: Knowledge acquisition and explanation for\n",
      "   multi-attribute decision making. In 8th Intl Workshop on Expert\n",
      "   Systems and their Applications, Avignon, France. pages 59-78, 1988.\n",
      "\n",
      "   Within machine-learning, this dataset was used for the evaluation\n",
      "   of HINT (Hierarchy INduction Tool), which was proved to be able to\n",
      "   completely reconstruct the original hierarchical model. This,\n",
      "   together with a comparison with C4.5, is presented in\n",
      "\n",
      "   B. Zupan, M. Bohanec, I. Bratko, J. Demsar: Machine learning by\n",
      "   function decomposition. ICML-97, Nashville, TN. 1997 (to appear)\n",
      "\n",
      "4. Relevant Information Paragraph:\n",
      "\n",
      "   Car Evaluation Database was derived from a simple hierarchical\n",
      "   decision model originally developed for the demonstration of DEX\n",
      "   (M. Bohanec, V. Rajkovic: Expert system for decision\n",
      "   making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates\n",
      "   cars according to the following concept structure:\n",
      "\n",
      "   CAR                      car acceptability\n",
      "   . PRICE                  overall price\n",
      "   . . buying               buying price\n",
      "   . . maint                price of the maintenance\n",
      "   . TECH                   technical characteristics\n",
      "   . . COMFORT              comfort\n",
      "   . . . doors              number of doors\n",
      "   . . . persons            capacity in terms of persons to carry\n",
      "   . . . lug_boot           the size of luggage boot\n",
      "   . . safety               estimated safety of the car\n",
      "\n",
      "   Input attributes are printed in lowercase. Besides the target\n",
      "   concept (CAR), the model includes three intermediate concepts:\n",
      "   PRICE, TECH, COMFORT. Every concept is in the original model\n",
      "   related to its lower level descendants by a set of examples (for\n",
      "   these examples sets see http://www-ai.ijs.si/BlazZupan/car.html).\n",
      "\n",
      "   The Car Evaluation Database contains examples with the structural\n",
      "   information removed, i.e., directly relates CAR to the six input\n",
      "   attributes: buying, maint, doors, persons, lug_boot, safety.\n",
      "\n",
      "   Because of known underlying concept structure, this database may be\n",
      "   particularly useful for testing constructive induction and\n",
      "   structure discovery methods.\n",
      "\n",
      "5. Number of Instances: 1728\n",
      "   (instances completely cover the attribute space)\n",
      "\n",
      "6. Number of Attributes: 6\n",
      "\n",
      "7. Attribute Values:\n",
      "\n",
      "   buying       v-high, high, med, low\n",
      "   maint        v-high, high, med, low\n",
      "   doors        2, 3, 4, 5-more\n",
      "   persons      2, 4, more\n",
      "   lug_boot     small, med, big\n",
      "   safety       low, med, high\n",
      "\n",
      "8. Missing Attribute Values: none\n",
      "\n",
      "9. Class Distribution (number of instances per class)\n",
      "\n",
      "   class      N          N[%]\n",
      "   -----------------------------\n",
      "   unacc     1210     (70.023 %) \n",
      "   acc        384     (22.222 %) \n",
      "   good        69     ( 3.993 %) \n",
      "   v-good      65     ( 3.762 %) \n"
     ]
    }
   ],
   "source": [
    "#shell scripts for downloading the data and placing it in a corresponding directory\n",
    "!mkdir CAR \n",
    "!curl -o CAR/data \"http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
    "!curl -o CAR/description \"http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names\"\n",
    "#download the description and display it here.\n",
    "!cat CAR/description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston house prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,\n",
       "        18.9,  15. ,  18.9,  21.7,  20.4,  18.2,  19.9,  23.1,  17.5,\n",
       "        20.2,  18.2,  13.6,  19.6,  15.2,  14.5,  15.6,  13.9,  16.6,\n",
       "        14.8,  18.4,  21. ,  12.7,  14.5,  13.2,  13.1,  13.5,  18.9,\n",
       "        20. ,  21. ,  24.7,  30.8,  34.9,  26.6,  25.3,  24.7,  21.2,\n",
       "        19.3,  20. ,  16.6,  14.4,  19.4,  19.7,  20.5,  25. ,  23.4,\n",
       "        18.9,  35.4,  24.7,  31.6,  23.3,  19.6,  18.7,  16. ,  22.2,\n",
       "        25. ,  33. ,  23.5,  19.4,  22. ,  17.4,  20.9,  24.2,  21.7,\n",
       "        22.8,  23.4,  24.1,  21.4,  20. ,  20.8,  21.2,  20.3,  28. ,\n",
       "        23.9,  24.8,  22.9,  23.9,  26.6,  22.5,  22.2,  23.6,  28.7,\n",
       "        22.6,  22. ,  22.9,  25. ,  20.6,  28.4,  21.4,  38.7,  43.8,\n",
       "        33.2,  27.5,  26.5,  18.6,  19.3,  20.1,  19.5,  19.5,  20.4,\n",
       "        19.8,  19.4,  21.7,  22.8,  18.8,  18.7,  18.5,  18.3,  21.2,\n",
       "        19.2,  20.4,  19.3,  22. ,  20.3,  20.5,  17.3,  18.8,  21.4,\n",
       "        15.7,  16.2,  18. ,  14.3,  19.2,  19.6,  23. ,  18.4,  15.6,\n",
       "        18.1,  17.4,  17.1,  13.3,  17.8,  14. ,  14.4,  13.4,  15.6,\n",
       "        11.8,  13.8,  15.6,  14.6,  17.8,  15.4,  21.5,  19.6,  15.3,\n",
       "        19.4,  17. ,  15.6,  13.1,  41.3,  24.3,  23.3,  27. ,  50. ,\n",
       "        50. ,  50. ,  22.7,  25. ,  50. ,  23.8,  23.8,  22.3,  17.4,\n",
       "        19.1,  23.1,  23.6,  22.6,  29.4,  23.2,  24.6,  29.9,  37.2,\n",
       "        39.8,  36.2,  37.9,  32.5,  26.4,  29.6,  50. ,  32. ,  29.8,\n",
       "        34.9,  37. ,  30.5,  36.4,  31.1,  29.1,  50. ,  33.3,  30.3,\n",
       "        34.6,  34.9,  32.9,  24.1,  42.3,  48.5,  50. ,  22.6,  24.4,\n",
       "        22.5,  24.4,  20. ,  21.7,  19.3,  22.4,  28.1,  23.7,  25. ,\n",
       "        23.3,  28.7,  21.5,  23. ,  26.7,  21.7,  27.5,  30.1,  44.8,\n",
       "        50. ,  37.6,  31.6,  46.7,  31.5,  24.3,  31.7,  41.7,  48.3,\n",
       "        29. ,  24. ,  25.1,  31.5,  23.7,  23.3,  22. ,  20.1,  22.2,\n",
       "        23.7,  17.6,  18.5,  24.3,  20.5,  24.5,  26.2,  24.4,  24.8,\n",
       "        29.6,  42.8,  21.9,  20.9,  44. ,  50. ,  36. ,  30.1,  33.8,\n",
       "        43.1,  48.8,  31. ,  36.5,  22.8,  30.7,  50. ,  43.5,  20.7,\n",
       "        21.1,  25.2,  24.4,  35.2,  32.4,  32. ,  33.2,  33.1,  29.1,\n",
       "        35.1,  45.4,  35.4,  46. ,  50. ,  32.2,  22. ,  20.1,  23.2,\n",
       "        22.3,  24.8,  28.5,  37.3,  27.9,  23.9,  21.7,  28.6,  27.1,\n",
       "        20.3,  22.5,  29. ,  24.8,  22. ,  26.4,  33.1,  36.1,  28.4,\n",
       "        33.4,  28.2,  22.8,  20.3,  16.1,  22.1,  19.4,  21.6,  23.8,\n",
       "        16.2,  17.8,  19.8,  23.1,  21. ,  23.8,  23.1,  20.4,  18.5,\n",
       "        25. ,  24.6,  23. ,  22.2,  19.3,  22.6,  19.8,  17.1,  19.4,\n",
       "        22.2,  20.7,  21.1,  19.5,  18.5,  20.6,  19. ,  18.7,  32.7,\n",
       "        16.5,  23.9,  31.2,  17.5,  17.2,  23.1,  24.5,  26.6,  22.9,\n",
       "        24.1,  18.6,  30.1,  18.2,  20.6,  17.8,  21.7,  22.7,  22.6,\n",
       "        25. ,  19.9,  20.8,  16.8,  21.9,  27.5,  21.9,  23.1,  50. ,\n",
       "        50. ,  50. ,  50. ,  50. ,  13.8,  13.8,  15. ,  13.9,  13.3,\n",
       "        13.1,  10.2,  10.4,  10.9,  11.3,  12.3,   8.8,   7.2,  10.5,\n",
       "         7.4,  10.2,  11.5,  15.1,  23.2,   9.7,  13.8,  12.7,  13.1,\n",
       "        12.5,   8.5,   5. ,   6.3,   5.6,   7.2,  12.1,   8.3,   8.5,\n",
       "         5. ,  11.9,  27.9,  17.2,  27.5,  15. ,  17.2,  17.9,  16.3,\n",
       "         7. ,   7.2,   7.5,  10.4,   8.8,   8.4,  16.7,  14.2,  20.8,\n",
       "        13.4,  11.7,   8.3,  10.2,  10.9,  11. ,   9.5,  14.5,  14.1,\n",
       "        16.1,  14.3,  11.7,  13.4,   9.6,   8.7,   8.4,  12.8,  10.5,\n",
       "        17.1,  18.4,  15.4,  10.8,  11.8,  14.9,  12.6,  14.1,  13. ,\n",
       "        13.4,  15.2,  16.1,  17.8,  14.9,  14.1,  12.7,  13.5,  14.9,\n",
       "        20. ,  16.4,  17.7,  19.5,  20.2,  21.4,  19.9,  19. ,  19.1,\n",
       "        19.1,  20.1,  19.9,  19.6,  23.2,  29.8,  13.8,  13.3,  16.7,\n",
       "        12. ,  14.6,  21.4,  23. ,  23.7,  25. ,  21.8,  20.6,  21.2,\n",
       "        19.1,  20.6,  15.2,   7. ,   8.1,  13.6,  20.1,  21.8,  24.5,\n",
       "        23.1,  19.7,  18.3,  21.2,  17.5,  16.8,  22.4,  20.6,  23.9,\n",
       "        22. ,  11.9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "dataset = datasets.load_boston()\n",
    "data, target = dataset.data, dataset.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data contains 14 columns. All columns description you can find below.\n",
    "\n",
    "* CRIM per capita crime rate by town\n",
    "* ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS proportion of non-retail business acres per town\n",
    "* CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* NOX nitric oxides concentration (parts per 10 million)\n",
    "* RM average number of rooms per dwelling\n",
    "* AGE proportion of owner-occupied units built prior to 1940\n",
    "* DIS weighted distances to five Boston employment centres\n",
    "* RAD index of accessibility to radial highways\n",
    "* TAX full-value property-tax rate per $ 10,000\n",
    "* PTRATIO pupil-teacher ratio by town\n",
    "\n",
    "\n",
    "B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "LSTAT  lower status of the population\n",
    "From data set description we can find that there are 13 continuous attributes (including “class” attribute “MEDV”) and 1 binary-valued attribute. There is no multiple categorical data, so we don’t need to change feature dimension. But we already have one problem. If you look closer, you will find that every column has its own data range. This situation is a bad thing for Neural Network training, because input values ​​make different contributions to the calculation of the output values. Bigger values will be more important for Network which can be perceived as invalid assumption based on data. For example in the first row, in the table above, column B contains value 396.90 and column CRIM - 0.00632. To fix this issue we should transfrom all columns to get similar ranges.\n",
    "\n",
    "source : \"http://neupy.com/2015/07/04/boston_house_prices_dataset.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database (Mixed National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments.[5] Furthermore, the black and white images from NIST were normalized to fit into a 20x20 pixel bounding box and anti-aliased, which introduced grayscale levels.\n",
    "The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset. There have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.23 percent. The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support vector machine to get an error rate of 0.8 percent.\n",
    "\n",
    "The following figure is an example taken from the MNIST digits.\n",
    "<img src=\"http://knowm.org/wp-content/uploads/Screen-Shot-2015-08-14-at-2.44.57-PM.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
